{
    "body": "Spack will avoid concretizing with a specific variant of an external package. In this particular example, I want to build Quantum Espresso with Open MPI 4.0.3 compiled with the GCC 10.1.0.\r\n\r\nHere is an excerpt from my `packages.yaml`:\r\n\r\n```yaml\r\npackages:\r\n  all:\r\n    compiler: [gcc, intel]\r\n  openmpi:\r\n    modules:\r\n      openmpi@4.0.3%intel@18.0.5:     mpi/openmpi/4.0.3/intel/2018\r\n      openmpi@4.0.3%intel@19.1.0.166: mpi/openmpi/4.0.3/intel/2020\r\n      openmpi@4.0.3%gcc@6.5.0:        mpi/openmpi/4.0.3/gcc/6.5.0\r\n      openmpi@4.0.3%gcc@8.4.0:        mpi/openmpi/4.0.3/gcc/8.4.0\r\n      openmpi@4.0.3%gcc@9.3.0:        mpi/openmpi/4.0.3/gcc/9.3.0\r\n      openmpi@4.0.3%gcc@10.1.0:       mpi/openmpi/4.0.3/gcc/10.1.0\r\n    buildable: true\r\n```\r\n\r\n### Steps to reproduce the issue\r\n\r\nSpack concretizes correctly for GCC 6.5.0, 8.4.0, and 9.3.0, but when requesting GCC 10.1.0, it selects `openmpi@4.0.3%gcc@9.3.0`. See the following examples:\r\n\r\n```console\r\n$ spack spec -I quantum-espresso %gcc@6.5.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@6.5.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@6.5.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@6.5.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@9.3.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@9.3.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@9.3.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@9.3.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@10.1.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@10.1.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@10.1.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@10.1.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n```\r\n\r\n### Error Message\r\n\r\nPer a suggestion from @becker33, I added\r\n\r\n```python\r\nprint(candidates)\r\nprint(spec)\r\n```\r\n\r\nto line 133 in `$SPACK/lib/spack/spack/concretize.py`. This produces the following for two attempts:\r\n\r\n```console\r\n$ spack spec -I quantum-espresso %gcc@6.5.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@6.5.0\r\nConcretized\r\n--------------------------------\r\n[quantum-espresso%gcc@6.5.0 ^blas ^fftw-api@3 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^blas ^fftw-api@3 ^lapack\r\n[intel-mkl@2020.0.166, intel-mkl@2018.5.274, intel-mkl, openblas, amdblis+blas, amdblis+cblas, atlas, blis+blas, blis+cblas, cray-libsci, essl, intel-parallel-studio+mkl, netlib-lapack~external-blas, netlib-xblas+plain_blas, veclibfort]\r\nblas\r\n[quantum-espresso%gcc@6.5.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166]\r\nquantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi\r\n[openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, intel-mpi@2020.0.166, intel-mpi@2018.5.274, openmpi@2.0.0:, openmpi@1.7.5:, openmpi@1.6.5, openmpi, mpich@3:, mpich@1:, mpich, mvapich2@2.3:, mvapich2@2.1:, mvapich2, fujitsu-mpi, intel-parallel-studio+mpi, mpilander, mpt@3:, mpt@1:, mpt, spectrum-mpi]\r\nmpi\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n -   quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none patches=b1aa3179ee1c069964fb9c21f3b832aebeae54947ce8d3cc1a74e7b154c3c10f arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@10.1.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@10.1.0\r\nConcretized\r\n--------------------------------\r\n[quantum-espresso%gcc@10.1.0 ^blas ^fftw-api@3 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^blas ^fftw-api@3 ^lapack\r\n[intel-mkl@2020.0.166, intel-mkl@2018.5.274, intel-mkl, openblas, amdblis+blas, amdblis+cblas, atlas, blis+blas, blis+cblas, cray-libsci, essl, intel-parallel-studio+mkl, netlib-lapack~external-blas, netlib-xblas+plain_blas, veclibfort]\r\nblas\r\n[quantum-espresso%gcc@10.1.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166]\r\nquantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi\r\n[openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, intel-mpi@2020.0.166, intel-mpi@2018.5.274, openmpi@2.0.0:, openmpi@1.7.5:, openmpi@1.6.5, openmpi, mpich@3:, mpich@1:, mpich, mvapich2@2.3:, mvapich2@2.1:, mvapich2, fujitsu-mpi, intel-parallel-studio+mpi, mpilander, mpt@3:, mpt@1:, mpt, spectrum-mpi]\r\nmpi\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n -   quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none patches=b1aa3179ee1c069964fb9c21f3b832aebeae54947ce8d3cc1a74e7b154c3c10f arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n```\r\n\r\n### Information on your system\r\n\r\n* **Spack:** 0.14.2-1529-ec58f28\r\n* **Python:** 3.6.8\r\n* **Platform:** linux-rhel7-haswell\r\n\r\nSee the excerpt from `packages.yaml` above.\r\n\r\n### Additional information\r\n\r\n- [x] I have run `spack debug report` and reported the version of Spack/Python/Platform\r\n- [x] I have searched the issues of this repo and believe this is not a duplicate\r\n- [ ] I have run the failing commands in debug mode and reported the output",
    "user": "roguephysicist",
    "url": "https://api.github.com/repos/spack/spack/issues/17227",
    "updated_at": "2020-12-09 19:46:18",
    "created_at": "2020-06-24 16:23:00",
    "closed_at": "2020-12-09 19:46:17",
    "state": "closed",
    "title": "Spack concretizer does not choose correct variant of external package.",
    "number": 17227,
    "milestone": null,
    "labels": [
        "bug",
        "triage"
    ],
    "id": 644746025,
    "html_url": "https://github.com/spack/spack/issues/17227",
    "assignees": [],
    "comments": 0
}