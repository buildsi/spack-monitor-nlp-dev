{
    "body": "### Steps to reproduce\n\nWorking on #26264 I had [this stacktrace](https://gitlab.spack.io/spack/spack/-/jobs/1127884) on Gitlab Pipelines. The stacktrace shows multiple processes trying a read operation and ending up trying to acquire a write lock (and timing out for that). While not easily reproducible, I think this is a bug at least in the API, since it is unexpected that a context manager that starts a \"read\" transaction ever need to acquire a write lock.\n\n### Error message\n\n```console\r\n[PID=232] LockTimeoutError: Timed out waiting for a write lock.\r\n5985TASK STACKTRACE:\r\n5986  File \"/builds/spack/spack/lib/spack/spack/environment/environment.py\", line 2012, in _concretize_task\r\n5987    value = _concretize_from_constraints(spec_constraints, tests)\r\n5988  File \"/builds/spack/spack/lib/spack/spack/environment/environment.py\", line 1989, in _concretize_from_constraints\r\n5989    return s.concretized(tests=tests)\r\n5990  File \"/builds/spack/spack/lib/spack/spack/spec.py\", line 2678, in concretized\r\n5991    clone.concretize(tests=tests)\r\n5992  File \"/builds/spack/spack/lib/spack/spack/spec.py\", line 2640, in concretize\r\n5993    self._new_concretize(tests)\r\n5994  File \"/builds/spack/spack/lib/spack/spack/spec.py\", line 2610, in _new_concretize\r\n5995    result = spack.solver.asp.solve([self], tests=tests)\r\n5996  File \"/builds/spack/spack/lib/spack/spack/solver/asp.py\", line 1691, in solve\r\n5997    return driver.solve(setup, specs, dump, models, timers, stats, tests)\r\n5998  File \"/builds/spack/spack/lib/spack/spack/solver/asp.py\", line 396, in solve\r\n5999    answers = builder.build_specs(tuples)\r\n6000  File \"/builds/spack/spack/lib/spack/spack/solver/asp.py\", line 1647, in build_specs\r\n6001    spack.spec.Spec.ensure_no_deprecated(s)\r\n6002  File \"/builds/spack/spack/lib/spack/spack/spec.py\", line 2586, in ensure_no_deprecated\r\n6003    with spack.store.db.read_transaction():\r\n6004  File \"/builds/spack/spack/lib/spack/llnl/util/lock.py\", line 685, in __enter__\r\n6005    self._as = self._acquire_fn()\r\n6006  File \"/builds/spack/spack/lib/spack/spack/database.py\", line 1054, in _read\r\n6007    with lk.WriteTransaction(self.lock):\r\n6008  File \"/builds/spack/spack/lib/spack/llnl/util/lock.py\", line 684, in __enter__\r\n6009    if self._enter() and self._acquire_fn:\r\n6010  File \"/builds/spack/spack/lib/spack/llnl/util/lock.py\", line 720, in _enter\r\n6011    return self._lock.acquire_write(self._timeout)\r\n6012  File \"/builds/spack/spack/lib/spack/llnl/util/lock.py\", line 448, in acquire_write\r\n6013    wait_time, nattempts = self._lock(fcntl.LOCK_EX, timeout=timeout)\r\n6014  File \"/builds/spack/spack/lib/spack/spack/util/lock.py\", line 32, in _lock\r\n6015    return super(Lock, self)._lock(op, timeout)\r\n6016  File \"/builds/spack/spack/lib/spack/llnl/util/lock.py\", line 316, in _lock\r\n6017    .format(lock_type[op]))\r\n```\r\n\n\n### Information on your system\n\nNot relevant. Happens in #26264 since it adds multiprocessing capabilities to concretization.\n\n### General information\n\n- [X] I have run `spack debug report` and reported the version of Spack/Python/Platform\n- [X] I have searched the issues of this repo and believe this is not a duplicate\n- [X] I have run the failing commands in debug mode and reported the output",
    "user": "alalazo",
    "url": "https://api.github.com/repos/spack/spack/issues/26600",
    "updated_at": "2021-10-08 22:35:24",
    "created_at": "2021-10-08 12:50:48",
    "closed_at": "2021-10-08 22:35:24",
    "state": "closed",
    "title": "spack.store.db.read_transaction might take a write lock",
    "number": 26600,
    "milestone": null,
    "labels": [
        "bug",
        "triage"
    ],
    "id": 1021063225,
    "html_url": "https://github.com/spack/spack/issues/26600",
    "assignees": [],
    "comments": 0
}