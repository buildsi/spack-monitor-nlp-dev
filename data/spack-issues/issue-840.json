{
    "body": "I've been having a bit of trouble using Spack-built versions of MVAPICH2 to compile MPI codes on the LC machines.  Specifically, I'm running into troubles getting basic MPI codes built with a Spack-provided MVAPICH2 to run with more than one task on [`cab`](https://computing.llnl.gov/?set=resources&page=OCF_resources#cab) (I'm able to compile and build without any troubles); the codes that I build in this environment fail to distribute MPI tasks properly and (consequently) execute the same code for each task.\n\nSince I'm able to compile and link the code, I'm fairly certain that the problem is related to the variant of MVAPICH2 that I've been using for my builds.  I've been trying `mvapich2@2.2b+slurm+psm` since the LC machines use SLURM for resource management and [`cab`](https://computing.llnl.gov/?set=resources&page=OCF_resources#cab) has QLogic infiniband interconnects, but I'm not entirely certain that this is correct.  Does this seem like the correct variant to be using or do I need to switch to a different resource manager/network type?  If this does seem like the correct variant, is there something else I should look into?  Also, for the LLNL folks (e.g. @tgamblin, @lee218llnl, @markcmiller86), have you been using MVAPICH2 and if so, what variant have you been using on machines like  [`cab`](https://computing.llnl.gov/?set=resources&page=OCF_resources#cab)?\n\nThanks in advance for any help that you can provide!\n",
    "user": "xjrc",
    "url": "https://api.github.com/repos/spack/spack/issues/840",
    "updated_at": "2016-06-06 19:09:22",
    "created_at": "2016-04-26 18:14:06",
    "closed_at": "2016-06-06 19:08:38",
    "state": "closed",
    "title": "[Question] Building MVAPICH2 for Use on QLogic Infiniband Networks",
    "number": 840,
    "milestone": null,
    "labels": [],
    "id": 151202816,
    "html_url": "https://github.com/spack/spack/issues/840",
    "assignees": [],
    "comments": 4
}