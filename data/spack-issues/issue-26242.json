{
    "body": "### Summary\r\n\r\nI believe that on x86_64 and other 2-way SMT architectures, spack should default to using all available CPU resources, as if called with the `-j $(nproc)` CLI parameter. Highly threaded CPU architectures like POWER, however might benefit from a cap on the amount of threads per core that is used. Benchmarking on such CPUs would be useful to clarify where the default cap should be set, but from x86 experience, it is probably safe to start by setting it to 2.\r\n\r\nThis proposal is directly challenging the point of view, previously stated in issue #11072 and implemented in PR #11373 that absolute build concurrency limits are a good thing. Some discussion with @jjwilke @mwkrentel and @citibeth would therefore be desirable before any decision is taken.\r\n\r\n### Rationale\r\n\r\nUnless I have missed something, the motivation behind spack's default cap of 16 build jobs, which is configurable via the `config:build_jobs` setting, is not stated clearly in the documentation.\r\n\r\nTo understand where this comes from, I therefore had to dig through the issue tracker to finally find issue #11072, which motivated the introduction of the 16-cpu cap through PR #11373.\r\n\r\nFrom my reading of this issue, two different rationales were invoked to justify this decision:\r\n1. Some C++ projects cannot build with all system CPUs because their build processes use up too much memory, leading to exhaustion of available system RAM\r\n2. Building with lots of processes on highly threaded CPUs like POWER chips yields diminishing returns anyway.\r\n\r\n---\r\n\r\nWith all due respect to @jjwilke, who originally formulated it, I think that the first rationale is misguided, in the sense that it blames on a build system a problem that actually originates from the software that is being built, and should therefore lie on the shoulder of said software's developers.\r\n\r\n<details>\r\n<summary>Here's why.</summary>\r\n\r\nIf one thinks for a second about the kind of work that a compiler does (translating and transforming abstract representations of the compilation unit that is being processed) and then ponders the volume of information that can be stored in a gigabyte of memory, it becomes readily apparent that when a compiler starts consuming multiple gigabytes of memory, it can only mean one of a few things:\r\n\r\n1. The compiler exhibits a major bug (e.g. memory leak) that must be tracked down and fixed.\r\n2. The compiler makes extraordinarily inefficient use of system resources and should be optimized.\r\n3. The compilation unit that is being processed is extremely large, and its contents should be investigated.\r\n\r\nFrom my experience, and that of other C++ developers fighting build bloat whose testimonies I've found across the Web, the third explanation is usually the right one. If one applies suitable tooling, such as clang's `-ftime-trace` or Templight, to a suspciously costly compilation units, one will almost always find dubious coding practices all across the codebase, which only end up showing in a specific compilation unit more than the others by means of accumulation. These include:\r\n\r\n- Undiscriminated use of inline function declarations in headers, in spite of the fact that for most functions this will result in lots of compilation work duplication with no significant associated runtime performance benefit.\r\n- Templated code, which must be inlined with most compilers, is written without any due attention paid to the fact that one copy of the code will be processed by the compiler for each template instantiation. As soon as this consideration is properly taken into account by developers, steps can be taken to reduce the associated code redundancy, revolving around the general idea of extracting the type-generic part of the templated code into a separate function (that may then be moved to a separate compilation unit if appropriate).\r\n- Templates are overused for use cases where they are an unnecessary evil and run-time polymorphism would yield more ergonomic APIs, reduced compilation cost, and comparable run-time performance.\r\n- Evil metaprogramming constructs are seemingly purpose-built to make compile-time code combinatorics explode without providing a matching benefit in exchange (my personal pet peeve here would be Eigen's use of expression templates, which actually provide little run-time performance benefit to match the compile-time combinatorial explosion that they introduce, in the small-matrix use cases that people most often use Eigen for).\r\n\r\nFrom this perspective, reducing build system concurrency in an attempt to avoid crashes caused by excessive compiler memory consumption is more often than not curing the symptom, rather than the disease. It is also a short-term fix in the sense that if the problem is not fixed at the source, which is lack of developer education and attention regarding build-time vs run-time performance tradeoffs, it will only get worse over time until it reaches the point where it becomes unbearable and real solutions need to be explored.\r\n\r\nThis point will usually be reached as soon as the code needs to be built on new machines whose resources are much more constrained than anything that was previously used. For example, the project may want to use public Github CI VMs (which are equipped with 2 cores and 7 GB of RAM), or a new developer/user may join and desire to build the project using a laptop whose specs are much weaker than what the other developers are used to.\r\n\r\nAnd from painful personal experience on a project where the largest compilation units individually reached 7 GB max-RSS and multi-minutes build time at the time where I had to intervene, it would be much better for everyone if the problem were acknowledged and tackled early on, rather than at the last moment where some people simply cannot work anymore until it is fixed.\r\n</details>\r\n\r\n---\r\n\r\nThe second rationale, on the other hand, I can agree with.\r\n\r\nHighly threaded CPU architectures are really meant to shine on IO-bound work, like typical web servers, where CPU execution resources are only lightly used and context switching performance is the name on the game. But from my experience, on typical hardware compilation is not quite I/O bound. It's mostly CPU-bound, it just happens that due to the way compilers are designed, the CPU is not used super-efficiently so there's room for another thread to steal a couple of cycles that would otherwise go to waste.\r\n\r\nTherefore, I think it makes sense to set a cap on the amount of **threads per physical core** that spack uses. The limit would need to be tuned based on some benchmarking on highly-threaded CPUs like POWER, but it could be initially set to 2 since it is well-known that on x86, hyperthreading leads to near-perfect scaling for compilation workloads, which leads to an expectation that SMT2 should work as well on POWER.\r\n\r\n---\r\n\r\nNow, beyond opposing other people's rationale, I must also state mine, so here goes:\r\n\r\n- Using all logical CPUs by default is the norm in all modern software construction tools (scons, ninja, cargo, bazel...). This creates a user expectation, which is violated by spack. As a result, spack's ergonomics suffer from this feature.\r\n- To make matters worse, spack's build job cap logic is not immediately obvious to users, as it is not put front and center in the documentation, and the associated behavior is pretty much invisible on the smaller personal machines where users are likely to start experimenting with spack. When these users later move to a beefier build node, they may fail to cross-check that spack uses all their CPU cores (and why would they ?), which will lead to fine compute resources going to waste and frustrated users wondering what's going on.\r\n\r\n### Description\r\n\r\nIf this proposal is accepted, the behavior of `config:build_jobs` would be reverted to where it was before PR #11373, i.e. no user-configured limit means no job concurrency limit at all.\r\n\r\nAs an alternative, a second configurable, which could perhaps be named `config:build_jobs_per_core`, would be introduced. This setting would default to 2 as a starting point, and might be tuned up later on if further benchmarking reveals that SMT-heavy compilation workloads scale better than expected here (and informally reported on #11072).\r\n\r\nSpack would honor this setting by setting a build jobs cap which is the maximum of `build_jobs` (if set) and `build_jobs_per_core` times the number of physical CPU cores.\r\n\r\n### Additional information\r\n\r\nThis problem exists on the current develop version of spack (`0.16.3-4390-b056e871f8`). It was introduced by PR #11373, in an attempt to fix issue #11072. While the topic of this issue is, in a sense, very close to that of #11072, I feel that so much time has passed and the point of view presented here is so different that necroposting by reopening #11072 might not be desirable.\r\n\r\n### General information\r\n\r\n- [X] I have run `spack --version` and reported the version of Spack\r\n- [X] I have searched the issues of this repo and believe this is not a duplicate",
    "user": "HadrienG2",
    "url": "https://api.github.com/repos/spack/spack/issues/26242",
    "updated_at": "2021-10-05 06:19:26",
    "created_at": "2021-09-24 18:32:33",
    "closed_at": "2021-10-05 06:19:26",
    "state": "closed",
    "title": "Proposal: Use 2 threads per CPU core by default",
    "number": 26242,
    "milestone": null,
    "labels": [
        "feature",
        "configuration",
        "discussion",
        "defaults",
        "performance",
        "proposal",
        "impact-medium",
        "user-experience",
        "expected behavior"
    ],
    "id": 1006726194,
    "html_url": "https://github.com/spack/spack/issues/26242",
    "assignees": [],
    "comments": 10
}